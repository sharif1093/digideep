import numpy as np
import random
from copy import deepcopy

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data

from digideep.agent.samplers.default import sampler_re
# from digideep.agent.samplers.default import check_shape

from digideep.utility.toolbox import get_class
from digideep.utility.logging import logger
from digideep.utility.profiling import KeepTime
from digideep.utility.monitoring import monitor


from .base import AgentBase
from digideep.agent.policy.deterministic import Policy

# torch.utils.backcompat.broadcast_warning.enabled = True

class DDPG(AgentBase):
    """This is an implementation of the Deep Deterministic Policy Gradient (`DDPG <https://arxiv.org/abs/1509.02971>`_) method.
    
    Args:
        name: The agent's name.
        type: The type of this class which is ``digideep.agent.DDPG``.
        methodargs (dict): The parameters of the DDPG method.
        sampler:
        policyname: The name of the policy which can be ``digideep.agent.policy.deterministic.Policy`` for normal DDPG.
        policyargs: The arguments for the policy.
        noisename: The noise model name.
        noiseargs: The noise model arguments.
        optimname: The name of the optimizer.
        optimargs: The arguments of the optimizer.
        
    The elements in the ``methodargs`` are:

    * ``n_update``: Number of times to perform DDPG step.
    * ``gamma``: Discount factor :math:`\gamma`.
    * ``clamp_return``: The clamp factor. One option is :math:`1/(1-\gamma)`.
    

    """

    def __init__(self, session, memory, **params):
        super(DDPG, self).__init__(session, memory, **params)

        self.device = self.session.get_device()

        # Set the Policy
        # policyclass = get_class(self.params["policyname"])
        self.policy = Policy(device=self.device, **self.params["policyargs"])
        
        
        # Set the optimizer (+ schedulers if any)
        optimclass_actor = get_class(self.params["optimname_actor"])
        optimclass_critic = get_class(self.params["optimname_critic"])
        self.optimizer = {}
        self.optimizer["actor"] = optimclass_actor(self.policy.model["actor"].parameters(), **self.params["optimargs_actor"])
        self.optimizer["critic"] = optimclass_critic(self.policy.model["critic"].parameters(), **self.params["optimargs_critic"])

        noiseclass = get_class(self.params["noisename"])
        self.noise = noiseclass(**self.params["noiseargs"])

        self.state["i_step"] = 0

    ###############
    ## SAVE/LOAD ##
    ###############
    # TODO: Also states of optimizers, noise, etc.
    def state_dict(self):
        return {'state':self.state, 'policy':self.policy.model.state_dict()}
    def load_state_dict(self, state_dict):
        self.policy.model.load_state_dict(state_dict['policy'])
        self.state.update(state_dict['state'])
    ############################################################
    
    def action_generator(self, observations, hidden_state, masks, deterministic=False):
        """This function computes the action based on observation, and adds noise to it if demanded.

        Args:
            deterministic (bool): If ``True``, the output would be merely the output from the actor network.
            Otherwise, noise will be added to the output actions.
        
        Returns:
            dict: ``{"actions":...,"hidden_state":...}``

        """
        observations_ = torch.from_numpy(observations).to(self.device)

        action = self.policy.generate_actions(observations_, deterministic=deterministic)
        action = action.cpu().data.numpy()

        if not deterministic:
            action = self.noise(action)

        results = dict(actions=action, hidden_state=hidden_state)
        return results


    def step(self):
        """This function needs the following key values in the batch of memory:

        * ``/observations``
        * ``/rewards``
        * ``/agents/<agent_name>/actions``
        * ``/observations_2``

        The first three keys are generated by the :class:`~digideep.environment.explorer.Explorer`
        and the last key is added by the sampler.
        """
        with KeepTime("/update/step/sampler"):
            info = deepcopy(self.params["sampler"])
            batch = sampler_re(data=self.memory, info=info)
            if batch is None:
                return


        with KeepTime("/update/step/to_torch"):
            # ['/observations', '/masks', '/agents/agent/actions', '/agents/agent/hidden_state', '/rewards', '/observations_2']
            
            o1 = torch.from_numpy(batch["/observations"]).to(self.device)
            r1 = torch.from_numpy(batch["/rewards"]).to(self.device)
            a1 = torch.from_numpy(batch["/agents/"+self.params["name"]+"/actions"]).to(self.device)
            o2 = torch.from_numpy(batch["/observations_2"]).to(self.device)

            # o1.clamp_(min=-self.params["trainer"]["clamp_obs"], max= self.params["trainer"]["clamp_obs"])
            # o2.clamp_(min=-self.params["trainer"]["clamp_obs"], max= self.params["trainer"]["clamp_obs"])


        with KeepTime("/update/step/loss/critic"):
            # ---------------------- optimize critic ----------------------
            # Use target actor exploitation policy here for loss evaluation
            a2 = self.policy.model["actor_target"](o2).detach()
            next_val = torch.squeeze(self.policy.model["critic_target"](o2, a2).detach())
            
            # y_target = r + gamma * Q'( s2, pi'(s2))
            # NOTE: THIS SENTENCE IS VERY IMPORTANT!
            r1 = torch.squeeze(r1)
            y_target = r1 + next_val * float(self.params["methodargs"]["gamma"])
            
            # TODO: IT WASN'T IN THE ORIGINAL IMPLEMENTATION BUT IN HER's.
            # y_target.clamp_(min=-self.params["methodargs"]["clamp_return"], max=0)

            # y_pred = Q( s1, a1)
            y_predicted = torch.squeeze(self.policy.model["critic"](o1, a1))
            # compute critic loss, and update the critic
            # smooth_l1_loss: Calculates l2 norm near zero and l1 elsewhere


            # NOTE: The following is in DDPG+HER implementation.
            # loss_critic = F.mse_loss(y_predicted, y_target, reduction='sum')
            # NOTE: The following was used in the original!
            loss_critic = F.smooth_l1_loss(y_predicted, y_target)

            self.optimizer["critic"].zero_grad()
            loss_critic.backward()
            self.optimizer["critic"].step()

        with KeepTime("/update/step/loss/actor"):
            # ---------------------- optimize actor ----------------------
            pred_a1 = self.policy.model["actor"](o1)

            loss_actor = -1 * torch.sum(self.policy.model["critic"](o1, pred_a1))
            self.optimizer["actor"].zero_grad()
            loss_actor.backward()
            self.optimizer["actor"].step()


        monitor("/update/loss_actor", loss_actor.item())
        monitor("/update/loss_critic", loss_critic.item())
        self.state["i_step"] += 1

    def update(self):
        # Update the networks for n times
        for i in range(self.params["methodargs"]["n_update"]):
            with KeepTime("/update/step"):
                self.step()
        
        with KeepTime("/update/targets"):
            # Update actor/critic targets
            self.policy.averager["actor"].update_target()
            self.policy.averager["critic"].update_target()
        
        ## For debugging
        # for p, ptar in zip(self.policy.model["actor"].parameters(), self.policy.model["actor_target"].parameters()):
        #     print(p.mean(), ptar.mean())
    
        # for p, ptar in zip(self.policy.model["actor"].parameters(), self.policy.model["critic"].parameters()):
        #     print(p.mean(), ptar.mean())


